{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RealPython.com Neural Networks Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first step in building a neural network is generating an output from input data. You’ll do that by creating a weighted sum of the variables\n",
    "\n",
    "In this first example, you have an input vector and the other two weight vectors. The goal is to find which of the weights is more similar to the input, taking into account the direction and the magnitude. If the weight is similar to the input, that would be the preferred weight\n",
    "\n",
    "First, you define the three vectors, one for the input and the other two for the weights. Then you compute how similar input_vector and weights_1 are. To do that, you’ll apply the dot product. Since all the vectors are two-dimensional vectors, these are the steps to do it:\n",
    "\n",
    "\n",
    "  1.  Multiply the first index of input_vector by the first index of weights_1.\n",
    "  2.  Multiply the second index of input_vector by the second index of weights_2.\n",
    "  3.  Sum the results of both multiplications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is: 2.1672\n"
     ]
    }
   ],
   "source": [
    "## Currently not using numpy for this block.\n",
    "\n",
    "#This is the code for computing the dot product of input_vector and weights_1:\n",
    "input_vector = [1.72, 1.23]\n",
    "input_vector2 = [2, 1.5]\n",
    "\n",
    "weights_1 = [1.26, 0]\n",
    "weights_2 = [2.17, 0.32]\n",
    "weights_3 = [1.45,-0.66]\n",
    "\n",
    "# Computing the dot product of input_vector and weights_1\n",
    "first_indexes_mult = input_vector[0] * weights_1[0]\n",
    "second_indexes_mult = input_vector[1] * weights_1[1]\n",
    "dot_product_1 = first_indexes_mult + second_indexes_mult\n",
    "\n",
    "print(f\"The dot product is: {dot_product_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the dot product is 2.1672. Now that you know how to compute the dot product, it’s time to use np.dot() from NumPy. Here’s how to compute dot_product_1 using np.dot():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is: 2.1672\n"
     ]
    }
   ],
   "source": [
    "# Calculating dot using numpy for this block\n",
    "import numpy as np\n",
    "\n",
    "dot_product_1 = np.dot(input_vector,weights_1)\n",
    "\n",
    "print(f\"The dot product is: {dot_product_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot() does the same thing you did before, but now you just need to specify the two arrays as arguments. Now let’s compute the dot product of input_vector and weights_2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product is: 4.1259999999999994\n"
     ]
    }
   ],
   "source": [
    "# Calculating the second dot_product of input_vector with weight_2 using numpy\n",
    "\n",
    "dot_product_2 = np.dot(input_vector,weights_2)\n",
    "\n",
    "print(f\"The dot product is: {dot_product_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot_product_2 would be the input and weight that are quite similar.\n",
    "\n",
    "As a different way of thinking about the dot product, you can treat the similarity between the vector coordinates as an on-off switch. If the multiplication result is 0, then you’ll say that the coordinates are not similar. If the result is something other than 0, then you’ll say that they are similar. \n",
    "\n",
    "This way, you can view the dot product as a loose measurement of similarity between the vectors. Every time the multiplication result is 0, the final dot product will have a lower result.\n",
    "\n",
    "since the dot product of input_vector and weights_2 is 4.1259, and 4.1259 is greater than 2.1672, it means that input_vector is more similar to weights_2. You’ll use this same mechanism in your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, you’ll train a model to make predictions that have only two possible outcomes. The output result can be either 0 or 1. This is a classification problem, a subset of supervised learning problems in which you have a dataset with the inputs and the known targets .The target is the variable you want to predict.**\n",
    "\n",
    "\"Usually, when there’s a need for a deep learning model, the data is presented in files, such as images or text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Your First Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to find an operation that makes the middle layers sometimes correlate with an input and sometimes not correlate. \n",
    "\n",
    "You can achieve this behavior by using nonlinear functions. These nonlinear functions are called activation functions.\n",
    "\n",
    "The network you’re building will use the sigmoid activation function. You’ll use it in the last layer, layer_2. The only two possible outputs in the dataset are 0 and 1, and the sigmoid function limits the output to a range between 0 and 1. \n",
    "\n",
    "Formula is S(x) = 1/(1+e^-x)\n",
    "\n",
    "The e is a mathematical constant called Euler’s number, and you can use np.exp(x) to calculate eˣ. \n",
    "\n",
    "Probability functions give you the probability of occurrence for possible outcomes of an event. The only two possible outputs of the dataset are 0 and 1, and the Bernoulli distribution is a distribution that has two possible outcomes as well. The sigmoid function is a good choice if your problem follows the Bernoulli distribution, so that’s why you’re using it in the last layer of your neural network.\n",
    "\n",
    "Since the function limits the output to a range of 0 to 1, you’ll use it to predict probabilities. If the output is greater than 0.5, then you’ll say the prediction is 1. If it’s below 0.5, then you’ll say the prediction is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now it’s time to turn all this knowledge into code. You’ll also need to wrap the vectors with NumPy arrays*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.7985731]\n"
     ]
    }
   ],
   "source": [
    "# Wrapping the vectors in NumPy arrays\n",
    "input_vector = np.array([1.66,1.56])\n",
    "weights_1 = np.array([1.45,-0.66])\n",
    "bias = np.array([0.0])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def make_prediction(input_vector, weights, bias):\n",
    "    layer_1 = np.dot(input_vector,weights) + bias\n",
    "    layer_2 = sigmoid(layer_1)\n",
    "    return layer_2\n",
    "\n",
    "prediction = make_prediction(input_vector,weights_1,bias)\n",
    "\n",
    "print(f\"The prediction result is: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw prediction result is 0.79, which is higher than 0.5, so the output is 1. The network made a correct prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now try it with another input vector, np.array([2, 1.5]). The correct result for this input is 0. You’ll only need to change the input_vector variable since all the other parameters remain the same*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.87101915]\n"
     ]
    }
   ],
   "source": [
    "# Changing the value of input_vector\n",
    "input_vector = np.array([2, 1.5])\n",
    "\n",
    "prediction = make_prediction(input_vector, weights_1, bias)\n",
    "\n",
    "print(f\"The prediction result is: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the network made a wrong prediction. The result should be less than 0.5 since the target for this input is 0, but the raw result was 0.87. It made a wrong guess, but how bad was the mistake? The next step is to find a way to assess that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Your First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of training the neural network, you first assess the error and then adjust the weights accordingly. To adjust the weights, you’ll use the gradient descent and backpropagation algorithms. Gradient descent is applied to find the direction and the rate to update the parameters. \n",
    "\n",
    "Before making any changes in the network, you need to compute the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Prediction Error\n",
    "\n",
    "To understand the magnitude of the error, you need to choose a way to measure it. The function used to measure the error is called the cost function, or loss function. \n",
    "\n",
    "you’ll use the mean squared error (MSE) as your cost function.\n",
    "\n",
    "You compute the MSE in two steps:\n",
    "\n",
    "    1. Compute the difference between the prediction and the target.\n",
    "    2. Multiply the result by itself.\n",
    "\n",
    "The network can make a mistake by outputting a value that’s higher or lower than the correct value. Since the MSE is the squared difference between the prediction and the correct result, with this metric you’ll always end up with a positive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the complete expression to compute the error for the last previous prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.87101915]; Error: [0.75867436]\n"
     ]
    }
   ],
   "source": [
    "target = 0\n",
    "\n",
    "mse = np.square(prediction - target)\n",
    "\n",
    "print(f\"Prediction: {prediction}; Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the error is 0.75. One implication of multiplying the difference by itself is that bigger errors have an even larger impact, and smaller errors keep getting smaller as they decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How to Reduce the Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to change the weights and bias variables so you can reduce the error. To understand how this works, you’ll change only the weights variable and leave the bias fixed for now. You can also get rid of the sigmoid function and use only the result of layer_1. All that’s left is to figure out how you can modify the weights so that the error goes down.\n",
    "\n",
    "You compute the MSE by doing error = np.square(prediction - target). If you treat (prediction - target) as a single variable x, then you have error = np.square(x), which is a quadratic function.\n",
    "\n",
    "The error is given by the y-axis. If you’re in point A and want to reduce the error toward 0, then you need to bring the x value down. On the other hand, if you’re in point B and want to reduce the error, then you need to bring the x value up. To know which direction you should go to reduce the error, you’ll use the derivative. A derivative explains exactly how a pattern will change. \n",
    "\n",
    "Another word for the derivative is **gradient**. **Gradient descent** is the name of the algorithm used to find the direction and the rate to update the network parameters. \n",
    "\n",
    "*In this tutorial, you won’t focus on the theory behind derivatives, so you’ll simply apply the derivative rules for each function you’ll encounter.* \n",
    "\n",
    "The power rule states that the derivative of xⁿ is nx⁽ⁿ⁻¹⁾. So the derivative of np.square(x) is 2 * x, and the derivative of x is 1.\n",
    "\n",
    "Remember that the error expression is error = np.square(prediction - target). When you treat (prediction - target) as a single variable x, the derivative of the error is 2 * x. By taking the derivative of this function, you want to know in what direction should you change x to bring the result of error to zero, thereby reducing the error.\n",
    "\n",
    "When it comes to your neural network, the derivative will tell you the direction you should take to update the weights variable. If it’s a positive number, then you predicted too high, and you need to decrease the weights. If it’s a negative number, then you predicted too low, and you need to increase the weights.\n",
    "\n",
    "If the mean squared error is 0.75, then should you increase or decrease the weights? Since the derivative is 2 * x, you just need to multiply the difference between the prediction and the target by 2\n",
    "\n",
    "*Now it’s time to write the code to figure out how to update weights_1 for the previous wrong prediction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative is [1.7420383]\n"
     ]
    }
   ],
   "source": [
    "derivative = 2 * (prediction - target)\n",
    "\n",
    "print(f\"The derivative is {derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 1.74, a positive number, so you need to decrease the weights. You do that by subtracting the derivative result of the weights vector.\n",
    "\n",
    "*Now you can update weights_1 accordingly and predict again to see how it affects the prediction result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.01496248]; Error: [0.00022388]\n"
     ]
    }
   ],
   "source": [
    "# Updating the weights\n",
    "weights_1 = weights_1 - derivative\n",
    "\n",
    "prediction = make_prediction(input_vector,weights_1,bias)\n",
    "\n",
    "error = (prediction - target) ** 2\n",
    "\n",
    "print(f\"Prediction: {prediction}; Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error dropped down to almost 0! Beautiful.\n",
    "\n",
    "*In this example, the derivative result was small, but there are some cases where the derivative result is too high.*\n",
    "\n",
    "Take the image of the quadratic function as an example. High increments aren’t ideal because you could keep going from point A straight to point B, never getting close to zero. To cope with that, you update the weights with a fraction of the derivative result. \n",
    "\n",
    "To define a fraction for updating the weights, you use the alpha parameter, also called the learning rate. If you decrease the learning rate, then the increments are smaller. If you increase it, then the steps are higher. How do you know what’s the best learning rate value? By making a guess and experimenting with it. \n",
    "\n",
    "**Traditional default learning rate values are 0.1, 0.01, and 0.001.**\n",
    "\n",
    "If you take the new weights and make a prediction with the first input vector, then you’ll see that now it makes a wrong prediction for that one.\n",
    "\n",
    "If your neural network makes a correct prediction for every instance in your training set, then you probably have an overfitted model, where the model simply remembers how to classify the examples instead of learning to notice features in the data. There are techniques to avoid that, including regularization the stochastic gradient descent.\n",
    "\n",
    "*In this tutorial you’ll use the online stochastic gradient descent.*\n",
    "\n",
    "*Now that you know how to compute the error and how to adjust the weights accordingly, it’s time to get back continue building your neural network.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your neural network, you need to update both the weights and the bias vectors. The function you’re using to measure the error depends on two independent variables, the weights and the bias. Since the weights and the bias are independent variables, you can change and adjust them to get the result you want.\n",
    "\n",
    "The network you’re building has two layers, and since each layer has its own functions, you’re dealing with a function composition. This means that the error function is still np.square(x), but now x is the result of another function.\n",
    "\n",
    "To restate the problem, now you want to know how to change weights_1 and bias to reduce the error. You already saw that you can use derivatives for this, but instead of a function with only a sum inside, now you have a function that produces its result using other functions.\n",
    "\n",
    "Since now you have this function composition, to take the derivative of the error concerning the parameters, you’ll need to use the chain rule from calculus. With the chain rule, you take the partial derivatives of each function, evaluate them, and multiply all the partial derivatives to get the derivative you want.\n",
    "\n",
    "Now you can start updating the weights. You want to know how to change the weights to decrease the error. This implies that you need to compute the derivative of the error with respect to weights. Since the error is computed by combining different functions, you need to take the partial derivatives of these functions. \n",
    "\n",
    "The bold red arrow shows the derivative you want, derror_dweights. You’ll start from the red hexagon, taking the inverse path of making a prediction and computing the partial derivatives at each function. \n",
    "\n",
    "*In the image above, each function is represented by the yellow hexagons, and the partial derivatives are represented by the gray arrows on the left. Applying the chain rule, the value of derror_dweights will be the following*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'derror_dweights = (\\n    derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\\n)'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''derror_dweights = (\n",
    "    derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To calculate the derivative, you multiply all the partial derivatives that follow the path from the error hexagon (the red one) to the hexagon where you find the weights (the leftmost green one).*\n",
    "\n",
    "You can say that the derivative of y = f(x) is the derivative of f with respect to x. Using this nomenclature, for derror_dprediction, you want to know the derivative of the function that computes the error with respect to the prediction value.\n",
    "\n",
    "This reverse path is called a **backward pass**. In each backward pass, you compute the partial derivatives of each function, substitute the variables by their values, and finally multiply everything. \n",
    "\n",
    "This “take the partial derivatives, evaluate, and multiply” part is how you apply the **chain rule**. This algorithm to update the neural network parameters is called **backpropagation**.\n",
    "\n",
    "Good for task 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Parameters With Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n this section, you’ll walk through the backpropagation process step by step, starting with how you update the bias. You want to take the derivative of the error function with respect to the bias, derror_dbias. Then you’ll keep going backward, taking the partial derivatives until you find the bias variable. \n",
    "\n",
    "Since you are starting from the end and going backward, you first need to take the partial derivative of the error with respect to the prediction. That’s the derror_dprediction\n",
    "\n",
    "The function that produces the error is a square function, and the derivative of this function is 2 * x.\n",
    "\n",
    "You applied the first partial derivative (derror_dprediction) and still didn’t get to the bias, so you need to take another step back and take the derivative of the prediction with respect to the previous layer, dprediction_dlayer1.\n",
    "\n",
    "The prediction is the result of the sigmoid function. You can take the derivative of the sigmoid function by multiplying sigmoid(x) and 1 - sigmoid(x). This derivative formula is very handy because you can use the sigmoid result that has already been computed to compute the derivative of it. You then take this partial derivative and continue going backward.\n",
    "\n",
    "Now you’ll take the derivative of layer_1 with respect to the bias\n",
    "\n",
    "here it is—you finally got to it! The bias variable is an independent variable, so the result after applying the power rule is 1.\n",
    "\n",
    "*now that you’ve completed this backward pass, you can put everything together and compute derror_dbias*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00044105])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return  sigmoid(x) * (1-sigmoid(x))\n",
    "    \n",
    "derror_deprediction = 2 * (prediction -target)\n",
    "layer_1 = np.dot(input_vector,weights_1) + bias\n",
    "dprediction_dlayer1 = sigmoid_deriv(layer_1)\n",
    "dlayer_dbias = 1\n",
    "\n",
    "derror_dbias = (derror_deprediction*dprediction_dlayer1*dlayer_dbias)\n",
    "derror_dbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights, you follow the same process, going backward and taking the partial derivatives until you get to the weights variable. Since you’ve already computed some of the partial derivatives, you’ll just need to compute dlayer1_dweights. The derivative of the dot product is the derivative of the first vector multiplied by the second vector, plus the derivative of the second vector multiplied by the first vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to write the expressions to update both the weights and the bias. It’s time to create a class for the neural network. Classes are the main building blocks of object-oriented programming (OOP). The NeuralNetwork class generates random start values for the weights and bias variables. \n",
    "\n",
    "When instantiating a NeuralNetwork object, you need to pass the learning_rate parameter. You’ll use predict() to make a prediction. The methods _compute_derivatives() and _update_parameters() have the computations you learned in this section. This is the final NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
    "        self.bias = np.random.rand()\n",
    "        self.learning_rate = learning_rate\n",
    "# Forward pass\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def _sigmoid_deriv(self,x):\n",
    "        return self._sigmoid(x) * (1-self._sigmoid(x))\n",
    "\n",
    "    def predict(self,input_vector):\n",
    "        layer_1 = np.dot(input_vector,self.weights) + bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "        return prediction\n",
    "# Back propagation\n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = np.dot(input_vector,self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2 * (prediction - target)\n",
    "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
    "        dlayer1_dbias = 1\n",
    "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
    "\n",
    "        derror_dbias = (\n",
    "            derror_deprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "        )\n",
    "\n",
    "        derror_dweights = (\n",
    "            derror_deprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        )\n",
    "\n",
    "        return derror_dbias, derror_dweights\n",
    "    \n",
    "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights -  (derror_dweights * self.learning_rate)\n",
    "    \n",
    "    def train(self, input_vectors, targets, iterations):\n",
    "        cumulative_errors = []\n",
    "        for current_iteration in range (iterations):\n",
    "            # Pick a data instance at random\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "            derror_dbias,derror_dweights = self._compute_gradients(\n",
    "                input_vector,target\n",
    "                )\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "            if current_iteration % 100 == 0:\n",
    "                cumulative_error = 0\n",
    "                # Loop through all the instances to measure the error\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "        \n",
    "        return cumulative_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s the code of your first neural network. Congratulations! This code just puts together all the pieces you’ve seen so far. If you want to make a prediction, first you create an instance of NeuralNetwork(), and then you call .predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00781746])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "neural_network = NeuralNetwork(learning_rate)\n",
    "\n",
    "neural_network.predict(input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code makes a prediction, but now you need to learn how to train the network.\n",
    "\n",
    "The goal is to make the network generalize over the training dataset. This means that you want it to adapt to new, unseen data that follow the same probability distribution as the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network With More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve already adjusted the weights and the bias for one data instance, but the goal is to make the network generalize over an entire dataset.\n",
    "\n",
    "Stochastic gradient descent is a technique in which, at every iteration, the model makes a prediction based on a randomly selected piece of training data, calculates the error, and updates the parameters.\n",
    "\n",
    "Now it’s time to create the train() method of your NeuralNetwork class.\n",
    "\n",
    "You’ll save the error over all data points every 100 iterations because you want to plot a chart showing how this metric changes as the number of iterations increases.\n",
    "\n",
    "This is the final train() method of your neural network. **It's been added to the above NeuralNetwork class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train(self, input_vectors, targets, iterations):\\n        cumulative_errors = []\\n        for current_iteration in range (iterations):\\n            # Pick a data instance at random\\n            random_data_index = np.random.randint(len(input_vectors))\\n\\n            input_vector = input_vectors[random_data_index]\\n            target = targets[random_data_index]\\n\\n            # Compute the gradients and update the weights\\n            derror_dbias,derror_dweights = self._compute_gradients(\\n                input_vector,target\\n                )\\n\\n            self._update_parameters(derror_dbias, derror_dweights)\\n\\n            # Measure the cumulative error for all the instances\\n            if current_iteration % 100 ==0:\\n                cumulative_error = 0\\n                # Loop through all the instances to measure the error\\n                for data_instance_index in range(len(input_vectors)):\\n                    data_point = input_vectors[data_instance_index]\\n                    target = targets[data_instance_index]\\n\\n                    prediction = self.predict(data_point)\\n                    error = np.square(prediction - target)\\n\\n                    cumulative_error = cumulative_error + error\\n                cumulative_errors.append(cumulative_error)\\n        \\n        return cumulative_errors'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''def train(self, input_vectors, targets, iterations):\n",
    "        cumulative_errors = []\n",
    "        for current_iteration in range (iterations):\n",
    "            # Pick a data instance at random\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "            derror_dbias,derror_dweights = self._compute_gradients(\n",
    "                input_vector,target\n",
    "                )\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "            if current_iteration % 100 ==0:\n",
    "                cumulative_error = 0\n",
    "                # Loop through all the instances to measure the error\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "        \n",
    "        return cumulative_errors'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a lot going on in the above code block, so here’s a line-by-line breakdown:\n",
    "\n",
    "    Line 8 picks a random instance from the dataset.\n",
    "\n",
    "    Lines 14 to 16 calculate the partial derivatives and return the derivatives for the bias and the weights. They use _compute_gradients(), which you defined earlier.\n",
    "\n",
    "    Line 18 updates the bias and the weights using _update_parameters(), which you defined in the previous code block.\n",
    "\n",
    "    Line 21 checks if the current iteration index is a multiple of 100. You do this to observe how the error changes every 100 iterations.\n",
    "\n",
    "    Line 24 starts the loop that goes through all the data instances.\n",
    "\n",
    "    Line 28 computes the prediction result.\n",
    "\n",
    "    Line 29 computes the error for every instance.\n",
    "\n",
    "    Line 31 is where you accumulate the sum of the errors using the cumulative_error variable. You do this because you want to plot a point with the error for all the data instances. Then, on line 32, you append the error to cumulative_errors, the array that stores the errors. You’ll use this array to plot the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, you pick a random instance from the dataset, compute the gradients, and update the weights and the bias.\n",
    "\n",
    "You also compute the cumulative error every 100 iterations and save those results in an array.\n",
    "\n",
    "*You’ll plot this array to visualize how the error changes during the training process.*\n",
    "\n",
    "*To keep things less complicated, you’ll use a dataset with just eight instances, the input_vectors array. Now you can call train() and use Matplotlib to plot the cumulative error for each iteration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error for all training instances')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_vectors = np.array([\n",
    "    [3, 1.5],\n",
    "    [2, 1],\n",
    "    [4, 1.5],\n",
    "    [3, 4],\n",
    "    [3.5, 0.5],\n",
    "    [2, 0.5],\n",
    "    [5.5, 1],\n",
    "    [1, 1],\n",
    "])\n",
    "\n",
    "targets = np.array([0, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "neural_network = NeuralNetwork(learning_rate)\n",
    "\n",
    "training_error = neural_network.train(input_vectors,targets,1000)\n",
    "\n",
    "plt.plot(training_error)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error for all training instances\")\n",
    "#plt.savefig(\"cumulative_error.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You instantiate the NeuralNetwork class again and call train() using the input_vectors and the target values. You specify that it should run 10000 times. This is the graph showing the error for an instance of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall error is decreasing, which is what you want. The image is generated in the same directory where you’re running IPython. After the largest decrease, the error keeps going up and down quickly from one interaction to another. That’s because the dataset is random and very small, so it’s hard for the neural network to extract any features.\n",
    "\n",
    "But it’s not a good idea to evaluate the performance using this metric because you’re evaluating it using data instances that the network already saw. This can lead to overfitting, when the model fits the training dataset so well that it doesn’t generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Layers to the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in this tutorial was kept small for learning purposes. Usually, deep learning models need a large amount of data because the datasets are more complex and have a lot of nuances.\n",
    "\n",
    "Since these datasets have more complex information, using only one or two layers isn’t enough. That’s why deep learning models are called “deep.” They usually have a large number of layers. \n",
    "\n",
    "By adding more layers and using activation functions, you increase the network’s expressive power and can make very high-level predictions. An example of these types of predictions is face recognition, such as when you take a photo of your face with your phone, and the phone unlocks if it recognizes the image as you"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be6f4628c426c520d00ae18c5a6c074786fdc298bf0fed48f49f1bcafb6f231e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
